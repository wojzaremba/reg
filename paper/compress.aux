\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{vanhoucke2011improving}
\citation{mathieu2013fast}
\citation{denil2013predicting}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Tensor Approximation Techniques}{2}{section.3}}
\newlabel{sec:approx_tech}{{3}{2}{Tensor Approximation Techniques}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Low-rank Approximations}{2}{subsection.3.1}}
\newlabel{subsec:low_rank}{{3.1}{2}{Low-rank Approximations}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Singular Value Decomposition for Matrices:}{3}{subsubsection.3.1.1}}
\newlabel{subsubsec:svd}{{3.1.1}{3}{Singular Value Decomposition for Matrices:}{subsubsection.3.1.1}{}}
\newlabel{svdapprox}{{1}{3}{Singular Value Decomposition for Matrices:}{equation.3.1}{}}
\newlabel{svdapproxerr}{{2}{3}{Singular Value Decomposition for Matrices:}{equation.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Singular Value Decomposition for Tensors:}{3}{subsubsection.3.1.2}}
\newlabel{subsubsec:svd_tensor}{{3.1.2}{3}{Singular Value Decomposition for Tensors:}{subsubsection.3.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Outer Product Decomposition for Tensors:}{3}{subsubsection.3.1.3}}
\newlabel{subsubsec:outer}{{3.1.3}{3}{Outer Product Decomposition for Tensors:}{subsubsection.3.1.3}{}}
\newlabel{eq:rank1}{{3}{3}{Outer Product Decomposition for Tensors:}{equation.3.3}{}}
\newlabel{eq:rankK}{{4}{3}{Outer Product Decomposition for Tensors:}{equation.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Clustering}{4}{subsection.3.2}}
\newlabel{subsec:clustering}{{3.2}{4}{Clustering}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Biclustering:}{4}{subsubsection.3.2.1}}
\newlabel{subsubsec:biclustering}{{3.2.1}{4}{Biclustering:}{subsubsection.3.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Balanced Clustering:}{4}{subsubsection.3.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Application to Convolutional Networks}{4}{section.4}}
\newlabel{sec:application}{{4}{4}{Application to Convolutional Networks}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Monochromatic Approximation}{4}{subsection.4.1}}
\newlabel{subsec:monochromatic}{{4.1}{4}{Monochromatic Approximation}{subsection.4.1}{}}
\newlabel{fig:common_convolution}{{1(a)}{5}{Subfigure 1(a)}{subfigure.1.1}{}}
\newlabel{sub@fig:common_convolution}{{(a)}{5}{Subfigure 1(a)\relax }{subfigure.1.1}{}}
\newlabel{fig:monochromatic}{{1(b)}{5}{Subfigure 1(b)}{subfigure.1.2}{}}
\newlabel{sub@fig:monochromatic}{{(b)}{5}{Subfigure 1(b)\relax }{subfigure.1.2}{}}
\newlabel{fig:biclustering}{{1(c)}{5}{Subfigure 1(c)}{subfigure.1.3}{}}
\newlabel{sub@fig:biclustering}{{(c)}{5}{Subfigure 1(c)\relax }{subfigure.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A visualization of convolution sparsity structures. The edges depict connections between input and output feature planes (the spatial extent of the planes are not shown). {\bf  (a):} regular convolution, where all input/output planes connect to one another. {\bf  (b):} The monochromatic approximation, used in the 1st layer. The input colors are projected to a set of intermediate color spaces, which are then shared by the output channels. Note the sparsity structure between the intermediate and output maps, which makes speedups feasible. {\bf  (c):} The biclustering procedure used for higher layers. Input and output planes are clustered into groups. Each group of planes is then approximated using a low-rank projection.}}{5}{figure.1}}
\newlabel{blo1}{{5}{5}{Monochromatic Approximation}{equation.4.5}{}}
\newlabel{blo2}{{6}{5}{Monochromatic Approximation}{equation.4.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Number of operations required for verious approximation methods.}}{6}{table.1}}
\newlabel{table:ops}{{1}{6}{Number of operations required for verious approximation methods}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Biclustering Approximations}{6}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Reconstruction metric}{6}{subsection.4.3}}
\newlabel{poormansmaha}{{7}{6}{Reconstruction metric}{equation.4.7}{}}
\newlabel{approxi}{{8}{6}{Reconstruction metric}{equation.4.8}{}}
\citation{imagenet}
\citation{sermanet2013overfeat}
\citation{zeiler2013visualizing}
\citation{eigenweb}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces $\ell _2$ reconstruction error of approximated weights in the original space (left) and the reweighted space (designed to match output error, see \ref  {poormansmaha}) (right), versus decrease in peformance for a range of different approximation hyperparameters. Markers with the same color use the same settings for $G,H$ but vary the approximation rank. The reweighted space makes the correlation between $l_2$ and classification error more linear (e.g.\nobreakspace  {}the red circles are well approximated by a line, but no so for the red crosses). Furthermore, for a given $l_2$ error, the performance loss is lower for the reweighted space. }}{7}{figure.2}}
\newlabel{fig:components}{{2}{7}{$\ell _2$ reconstruction error of approximated weights in the original space (left) and the reweighted space (designed to match output error, see \ref {poormansmaha}) (right), versus decrease in peformance for a range of different approximation hyperparameters. Markers with the same color use the same settings for $G,H$ but vary the approximation rank. The reweighted space makes the correlation between $l_2$ and classification error more linear (e.g.~the red circles are well approximated by a line, but no so for the red crosses). Furthermore, for a given $l_2$ error, the performance loss is lower for the reweighted space}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{7}{section.5}}
\newlabel{sec:experiments}{{5}{7}{Experiments}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Speedup}{7}{subsection.5.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Evaluation time in seconds per layer of MattNet on CPU (left) and GPU (right) with batch size of 128. Results are averaged over 8 runs.}}{8}{table.2}}
\newlabel{evaluation_time}{{2}{8}{Evaluation time in seconds per layer of MattNet on CPU (left) and GPU (right) with batch size of 128. Results are averaged over 8 runs}{table.2}{}}
\newlabel{fig:RGB_components}{{5.1.1}{8}{First Layer}{subsubsection.5.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Visualization of the 1st layer filters in MattNet. Each component of the 96 7x7 filters is plotted in RGB space. Points are colored based on the output filter they belong to. Hence, there are 96 colors and $7^2$ points of each color. ({\bf  Left}) Shows the original filters and ({\bf  Right}) shows the filters after the monochromatic approximation, where each filter has been projected down to a line in colorspace.}}{8}{figure.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}First Layer}{8}{subsubsection.5.1.1}}
\newlabel{fig:denoising}{{5.1.1}{9}{First Layer}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Original and approximate versions (using 12 colors) of 1st later filters from MattNet.}}{9}{figure.4}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Performance of MattNet when first layer weights are replaced with monochromatic approximation and the corresponding theoretical speedup. Classification error on 8K validation images tends to increase as the approximation becomes harsher (i.e. fewer colors are used). Theoretical speedups vary only slightly as the number of colors used increases since the color transformation contributes relatively little to the total number of operations.}}{9}{table.3}}
\newlabel{table:mono_perf}{{3}{9}{Performance of MattNet when first layer weights are replaced with monochromatic approximation and the corresponding theoretical speedup. Classification error on 8K validation images tends to increase as the approximation becomes harsher (i.e. fewer colors are used). Theoretical speedups vary only slightly as the number of colors used increases since the color transformation contributes relatively little to the total number of operations}{table.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Second Layer}{9}{subsubsection.5.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Empirical speedups on ({\bf  Left}) CPU and ({\bf  Right}) GPU for the first layer of MattNet.}}{10}{figure.5}}
\newlabel{fig:mono_speedups}{{5}{10}{Empirical speedups on ({\bf Left}) CPU and ({\bf Right}) GPU for the first layer of MattNet}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Theoretically achievable speedups with various biclustering approximations applied to ({\bf  Left}) MattNet and ({\bf  Right}) PierreNet.}}{10}{figure.6}}
\newlabel{fig:biclustering_theory}{{6}{10}{Theoretically achievable speedups with various biclustering approximations applied to ({\bf Left}) MattNet and ({\bf Right}) PierreNet}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Reduction in memory overhead}{10}{subsection.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{10}{section.6}}
\citation{*}
\bibstyle{splncs}
\bibdata{bibliography}
\bibcite{vanhoucke2011improving}{1}
\bibcite{mathieu2013fast}{2}
\bibcite{denil2013predicting}{3}
\bibcite{imagenet}{4}
\bibcite{sermanet2013overfeat}{5}
\bibcite{zeiler2013visualizing}{6}
\bibcite{eigenweb}{7}
\bibcite{zeiler2011adaptive}{8}
\bibcite{LeNgiChenChiaKohNg10}{9}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Empirical speedups on ({\bf  Left}) CPU and ({\bf  Right}) GPU for the second layer of MattNet.}}{11}{figure.7}}
\newlabel{fig:biclust_speedups}{{7}{11}{Empirical speedups on ({\bf Left}) CPU and ({\bf Right}) GPU for the second layer of MattNet}{figure.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Number of parameters expressed as a function of hyperparameters for various approximation methods.}}{11}{table.4}}
\newlabel{table:memory_theory}{{4}{11}{Number of parameters expressed as a function of hyperparameters for various approximation methods}{table.4}{}}
\bibcite{le2011building}{10}
\bibcite{lowe1999object}{11}
\bibcite{hinton2012improving}{12}
\bibcite{krizhevsky2012imagenet}{13}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Empirical reduction in parameters for first two layers of MattNet with corresponding network performance.}}{12}{table.5}}
\newlabel{table:memory_empirical}{{5}{12}{Empirical reduction in parameters for first two layers of MattNet with corresponding network performance}{table.5}{}}
