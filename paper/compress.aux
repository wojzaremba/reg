\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\citation{sermanet2013overfeat}
\citation{denil2013predicting}
\citation{hinton2012improving}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{vanhoucke2011improving}
\citation{mathieu2013fast}
\citation{denil2013predicting}
\citation{zisserman14}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}}
\newlabel{relwork}{{2}{2}{Related Work\relax }{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Convolutional Tensor Compression}{2}{section.3}}
\newlabel{sec:approx_tech}{{3}{2}{Convolutional Tensor Compression\relax }{section.3}{}}
\citation{zisserman14}
\citation{zisserman14}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Approximation Metric}{3}{subsection.3.1}}
\newlabel{reconstr_sect}{{3.1}{3}{Approximation Metric\relax }{subsection.3.1}{}}
\newlabel{approxi}{{1}{3}{Approximation Metric\relax }{equation.1}{}}
\newlabel{poormansmaha}{{2}{3}{Approximation Metric\relax }{equation.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Low-rank Tensor Approximations}{3}{subsection.3.2}}
\newlabel{subsec:low_rank}{{3.2}{3}{Low-rank Tensor Approximations\relax }{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Matrix Decomposition}{3}{subsubsection.3.2.1}}
\newlabel{subsubsec:svd}{{3.2.1}{3}{Matrix Decomposition\relax }{subsubsection.3.2.1}{}}
\citation{rankonetensors}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Higher Order Tensor Approximations}{4}{subsubsection.3.2.2}}
\newlabel{subsubsec:svd_tensor}{{3.2.2}{4}{Higher Order Tensor Approximations\relax }{subsubsection.3.2.2}{}}
\newlabel{eq:rank1}{{4}{4}{Higher Order Tensor Approximations\relax }{equation.4}{}}
\newlabel{eq:rankK}{{5}{4}{Higher Order Tensor Approximations\relax }{equation.5}{}}
\newlabel{fig:monochromatic}{{1(a)}{4}{Subfigure 1(a)\relax }{subfigure.1.1}{}}
\newlabel{sub@fig:monochromatic}{{(a)}{4}{Subfigure 1(a)\relax }{subfigure.1.1}{}}
\newlabel{fig:biclustering}{{1(b)}{4}{Subfigure 1(b)\relax }{subfigure.1.2}{}}
\newlabel{sub@fig:biclustering}{{(b)}{4}{Subfigure 1(b)\relax }{subfigure.1.2}{}}
\newlabel{fig:monochromatic}{{1(c)}{4}{Subfigure 1(c)\relax }{subfigure.1.3}{}}
\newlabel{sub@fig:monochromatic}{{(c)}{4}{Subfigure 1(c)\relax }{subfigure.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  A visualization of monochromatic and biclustering approximation structures. {\bf  (a)} The monochromatic approximation, used for the first layer. Input color channels are projected onto a set of intermediate color channels. After this transformation, output features need only to look at one intermediate color channel. {\bf  (b)} The biclustering approximation, used for higher convolution layers. Input and output features are clustered into equal sized groups. The weight tensor corresponding to each pair of input and output clusters is then approximated. {\bf  (c)} The weight tensors for each input-output pair in (b) are approximated by a sum of rank 1 tensors using techniques described in \ref  {subsubsec:svd_tensor}}}{4}{figure.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Monochromatic Convolution Approximation}{4}{subsection.3.3}}
\newlabel{subsec:monochromatic}{{3.3}{4}{Monochromatic Convolution Approximation\relax }{subsection.3.3}{}}
\citation{zeiler2013visualizing}
\citation{imagenet}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Number of operations required for various approximation methods.}}{5}{table.1}}
\newlabel{table:ops}{{1}{5}{Number of operations required for various approximation methods}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Biclustering Approximations}{5}{subsection.3.4}}
\newlabel{subsec:clustering}{{3.4}{5}{Biclustering Approximations\relax }{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Fine-tuning}{5}{subsection.3.5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{5}{section.4}}
\newlabel{sec:experiments}{{4}{5}{Experiments\relax }{section.4}{}}
\citation{eigenweb}
\newlabel{fig:RGB_components}{{4.1.1}{6}{First Layer\relax }{subsubsection.4.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visualization of the 1st layer filters. {\bf  (Left)} Each component of the 96 7x7 filters is plotted in RGB space. Points are colored based on the output filter they belong to. Hence, there are 96 colors and $7^2$ points of each color. Leftmost plot shows the original filters and the right plot shows the filters after the monochromatic approximation, where each filter has been projected down to a line in colorspace. {\bf  (Right)} Original and approximate versions of a selection of 1st layer filters.}}{6}{figure.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Speedup}{6}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}First Layer}{6}{subsubsection.4.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Empirical speedups on ({\bf  Left}) CPU and ({\bf  Right}) GPU for the first layer. $C'$ is the number of colors used in the approximation.}}{7}{figure.3}}
\newlabel{fig:mono_speedups}{{3}{7}{Empirical speedups on ({\bf Left}) CPU and ({\bf Right}) GPU for the first layer. $C'$ is the number of colors used in the approximation}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Empirical speedups for second convolutional layer. ({\bf  Left}) Speedups on CPU using biclustered ($G = 2$ and $H = 2$) with SVD approximation. ({\bf  Right}) peedups on GPU using biclustered ($G = 48$ and $h = 2$) with outer product decomposition approximation.}}{7}{figure.4}}
\newlabel{fig:biclust_speedups}{{4}{7}{Empirical speedups for second convolutional layer. ({\bf Left}) Speedups on CPU using biclustered ($G = 2$ and $H = 2$) with SVD approximation. ({\bf Right}) peedups on GPU using biclustered ($G = 48$ and $h = 2$) with outer product decomposition approximation}{figure.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Second Layer}{7}{subsubsection.4.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Combining approximations}{7}{subsection.4.2}}
\citation{*}
\bibstyle{splncs}
\bibdata{bibliography}
\bibcite{sermanet2013overfeat}{1}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Number of parameters expressed as a function of hyperparameters for various approximation methods and empirical reduction in parameters with corresponding network performance.}}{8}{table.2}}
\newlabel{table:memory}{{2}{8}{Number of parameters expressed as a function of hyperparameters for various approximation methods and empirical reduction in parameters with corresponding network performance}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Reduction in memory overhead}{8}{subsection.4.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{8}{section.5}}
\bibcite{denil2013predicting}{2}
\bibcite{hinton2012improving}{3}
\bibcite{vanhoucke2011improving}{4}
\bibcite{mathieu2013fast}{5}
\bibcite{zisserman14}{6}
\bibcite{rankonetensors}{7}
\bibcite{zeiler2013visualizing}{8}
\bibcite{imagenet}{9}
\bibcite{eigenweb}{10}
\bibcite{zeiler2011adaptive}{11}
\bibcite{LeNgiChenChiaKohNg10}{12}
\bibcite{le2011building}{13}
\bibcite{lowe1999object}{14}
\bibcite{krizhevsky2012imagenet}{15}
