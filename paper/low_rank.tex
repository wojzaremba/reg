\section{Convolutional Tensor Compression}\label{sec:approx_tech}
%In typical object recognition architectures, the weights of
%convolutional layers at the end of training exhibit strong redundancy
%and regularity across all dimensions. 
In this section we describe
techniques for compressing 
4 dimensional convolutional weight tensors and fully connected weight matrices into a representation that permits
efficient computation and storage. 
Section \ref{reconstr_sect} describes how to construct a good approximation 
criteria. Section \ref{subsec:low_rank} describes techniques for low-rank tensor
approximations. Sections \ref{subsec:monochromatic} and \ref{subsec:clustering} describe how to 
apply these techniques to approximate weights of a convolutional neural network.

%In this section we describe how to use the techniques described in
%Section \ref{sec:approx_tech} to approximate covolutional weight
%tensors in ways that allow for a more efficient computation of the
%convolution. The approximations are more efficient in two aspects:
%both the number of floating point operations required to compute the
%convolution output and the number of parameters that need to be stored
%are dramatically reduced.

\subsection{Approximation Metric}
\label{reconstr_sect}

Our goal is to find an approximation, $\tilde{W}$, of a convolutional tensor $W$ 
that facilitates more efficient computation while maintaining the prediction performance of the network.
A natural choice for an approximation criterion is
to minimize $\| \tilde{W} - W \|_F$. This criterion yields 
efficient compression schemes using elementary linear algebra, and also controls
the operator norm of each linear convolutional layer.
%
%Euclidean distance has the advantage that then finding low-rank 
%approximations can be solved explicitly with efficient algorithms. 
However, this criterion assumes that all directions in the space of weights equally 
affect prediction performance. We now present two methods of improving this criterion while 
keeping the same efficient approximation algorithms.

{\bf Mahalanobis distance metric: }
The first distance metric we propose seeks to emphasize coordinates more prone to produce prediction errors over 
coordinates whose effect is less harmful for the overall system. 
We can obtain such measurements as follows.
Let $\Theta=\{W_1,\dots,W_S\}$ denote
the set of all parameters of the $S$-layer network, and let $U(I; \Theta)$ denote the output 
after the softmax layer of input image $I$.
We consider  a given input training set $(I_1,\dots,I_N)$ 
with known labels $(y_1,\dots,y_N)$. For each pair $(I_n, y_n)$, 
we compute the forward propagation pass $U(I_n, \Theta)$, and 
define as $\{\beta_n\}$ the indices of the $h$ largest values of  $U(I_n, \Theta)$ 
different from $y_n$.
Then, for a given layer $s$, we compute
\begin{equation}
\label{approxi}
d_{n,l,s} = \nabla_{W_s} \left( U(I_n, \Theta) - \delta(i - l)\right)~,~n\leq N\,,\, l \in \{\beta_n\}\,,\, s\leq S~,
\end{equation}
where $\delta(i-l)$ is the dirac distribution centered at $l$.
In other words, for each input we back-propagate the difference between the current prediction and the 
$h$ ``most dangerous" mistakes. 

The Mahalanobis distance is defined from the covariance of 
$d$: $\| W \|_{maha}^2 = w \Sigma^{-1} w^T~,$
where $w$ is the vector containing all the coordinates of $W$, and $\Sigma$ is the covariance of $(d_{n,l,s})_{n,l}$. 
We do not report results using this metric, since it requires inverting a matrix of size equal to the number 
of parameters, which can be prohibitively expensive in large networks.
Instead we use an approximation that considers only the diagonal of the covariance matrix. 
In particular, we propose the following, approximate, Mahalanobis distance metric: 
\begin{equation}
\label{poormansmaha}
\| W \|_{\widetilde{maha}} := \sum_p \alpha_p W(p) ~,\text{ where } \alpha_p = \Big( \sum_{n,l} d_{n,l,s}(p)^2 \Big)^{1/2}~
\end{equation}
where the sum runs over the tensor coordinates.
Since (\ref{poormansmaha}) is a reweighted Euclidiean metric, we 
can simply compute $W' = \alpha~ .* W$, where $.*$ denotes element-wise multiplication, 
then compute the approximation $\tilde{W'}$ on $W'$ using the standard $L_2$ norm, 
and finally output $\tilde{W} = \alpha^{-1} .* \tilde{W'}~.$

{\bf Data covariance distance metric: }
One can view the Frobenius norm of $W$ as
$\| W \|_F^2 = \mathbb{E}_{x \sim \mathcal{N}(0,I)} \| W x \|_F^2 ~.$
Another alternative, similar to the one considered in \cite{zisserman14}, is to replace the isotropic covariance
assumption by the empirical covariance of the input of the layer. If $W \in \mathbb{R}^{C \times X \times Y \times F}$ 
is a convolutional layer, and $\widehat{\Sigma} \in \mathbb{R}^{CXY \times CXY}$ is the empirical estimate of the input data covariance, 
it can be efficiently computed as 
\begin{equation}
\| W \|_{data} = \| \widehat{\Sigma}^{1/2} W_F \|_F~, 
\end{equation}
where $W_F$ is the matrix obtained by folding the first three dimensions of $W$.As opposed to \cite{zisserman14}, this
approach adapts to the input distribution without the need to iterate through the data.
\subsection{Low-rank Tensor Approximations}\label{subsec:low_rank}

%A particularly simple strategy to exploit the redundancy present in trained convolutional network weights is to 
%linearly compress the tensors, which amounts to finding low-rank approximations. 

%\vspace{-0.3cm}
\subsubsection{Matrix Decomposition}\label{subsubsec:svd}
Matrices are $2$-tensors which can be linearly compressed using the Singular Value Decomposition. 
If $W \in \mathbb{R}^{m \times k}$ is a real matrix, the SVD is defined as
%\begin{equation*}
	$W = USV^{\top}$, where $U \in \mathbb{R}^{m \times m}, S \in \mathbb{R}^{m \times k}, V \in \mathbb{R}^{k \times k}$.
%\end{equation*}
$S$ is a diagonal matrix with the singular values on the diagonal, and $U$, $V$ are orthogonal matrices. 
If the singular values of $W$ decay rapidly, $W$ can be well approximated by keeping only the $t$ largest entries of $S$, 
resulting in the approximation 
%We can write the approximation as
%\begin{equation}
%\label{svdapprox}
	$\tilde{W} = \tilde{U}\tilde{S}\tilde{V}^{\top}$, where $\tilde{U} \in \mathbb{R}^{m \times t}, \tilde{S} \in \mathbb{R}^{t \times t}, \tilde{V} \in \mathbb{R}^{t \times k}$
%\end{equation}
Then, for $I \in \mathbb{R}^{n \times m}$, the approximation error $\| I \tilde{W} - I W \|_F$ satisfies 
%\begin{equation}
%\label{svdapproxerr}
  $\| I \tilde{W} - I W \|_F \leq s_{t+1} \| I \|_F~,$
%\end{equation}
and thus is controlled by the decay along the diagonal of $S$.
Now the computation $I\tilde{W}$ can be done in $O(nmt + nt^2 + ntk)$, which, for sufficiently small $t$ is significantly smaller than $O(nmk)$. 

%\vspace{-0.3cm}
%\subsubsection{Low Rank Tensor Approximations}\label{subsubsec:svd_tensor}
\subsubsection{Higher Order Tensor Approximations}\label{subsubsec:svd_tensor}
SVD can be used to approximate a tensor $W \in \mathbb{R}^{m \times n \times k}$
by first folding all but two dimensions together to convert it into a $2$-tensor, %eg $W_f \in \mathbb{R}^{m \times (n \cdot k)}$, 
and then considering the SVD of the resulting matrix. For example, we can approximate $W_m \in \mathbb{R}^{m \times (nk)}$ as $\tilde{W}_m \approx \tilde{U}\tilde{S}\tilde{V}^{\top}$. 
$W$ can be compressed even further by applying SVD to $\tilde{V}$. We refer to this approximation as the SVD decomposition and use $K_1$ and $K_2$ to denote the rank used in the first and second application of SVD respectively.

Alternatively, we can approximate a 3-tensor, $W_S \in \mathbb{R}^{m \times n \times k}$, by a rank 1 3-tensor by finding a decomposition that minimizes 
\begin{equation}
\label{eq:rank1}
	\| W - \alpha \otimes \beta \otimes \gamma \|_F~,
\end{equation} 
where $\alpha \in \mathbb{R}^m$, $\beta \in \mathbb{R}^{n}$, $\gamma \in \mathbb{R}^k$ and $\otimes$ denotes the outer product operation.
Problem (\ref{eq:rank1}) is solved efficiently by performing alternate least squares 
on $\alpha$, $\beta$ and $\gamma$ respectively, although more efficient algorithms can also be 
considered \cite{rankonetensors}. 

This easily extends to a rank $K$ approximation using a greedy algorithm: Given a 
tensor $W$, we compute $(\alpha, \beta, \gamma)$ using (\ref{eq:rank1}), and we update 
$W^{(k+1)} \leftarrow W^{k} - \alpha \otimes \beta \otimes \gamma$. Repeating this operation $K$
times results in 
\begin{equation}
\label{eq:rankK}
	\tilde{W_S} = \sum_{k = 1}^{K} \alpha_k \otimes \beta_k \otimes \gamma_k ~.
\end{equation} 
We refer to this approximation as the outer product decomposition and use $K$ to denote the rank of the approximation.

\vspace{-3mm}
\begin{figure}[ht]
\centering
\mbox{
%\hspace{5mm}
\subfigure[][]{
  \includegraphics[width=0.3\linewidth]{img/monochromatic_illustration.pdf} 
  \label{fig:monochromatic}
}
\hspace{1mm}
\subfigure[][]{
  \includegraphics[width=0.3\linewidth]{img/bicluster_illustration.pdf} 
  \label{fig:biclustering}
}
\hspace{1mm}
\subfigure[][]{
  \includegraphics[width=0.3\linewidth]{img/tensor_sum.pdf} 
  \label{fig:monochromatic}
}
}
\vspace{-3mm}
\caption{ A visualization of monochromatic and biclustering approximation structures. {\bf (a)} The monochromatic approximation, used for the first layer. Input color channels are projected onto a set of intermediate color channels. After this transformation, output features need only to look at one intermediate color channel. {\bf (b)} The biclustering approximation, used for higher convolution layers. Input and output features are clustered into equal sized groups. The weight tensor corresponding to each pair of input and output clusters is then approximated. {\bf (c)} The weight tensors for each input-output pair in (b) are approximated by a sum of rank 1 tensors using techniques described in \ref{subsubsec:svd_tensor}}
\end{figure}

\subsection{Monochromatic Convolution Approximation}\label{subsec:monochromatic}
Let $W \in \mathbb{R}^{C \times X \times Y \times F}$ denote the
weights of the first convolutional layer of a trained network.  
We found that the color components
 of trained CNNs tend to have low dimensional structure. In
particular, the weights can be well approximated by projecting the
color dimension down to a 1D subspace.
The low-dimensional structure of the weights is illustrated in Figure
\ref{fig:RGB_components}.

The monochromatic approximation exploits this structure and is computed as follows.
First, for every output feature, $f$, we consider consider the matrix $W_f \in \mathbb{R}^{C \times (XY) }$, 
where the spatial dimensions of the filter corresponding to the output feature have been combined, and find the SVD, 
$W_f = U_f S_f V_f^{\top}$,
%\begin{equation*}
%	W_f = U_f S_f V_f^{\top}
%\end{equation*}
where $U_f \in \mathbb{R}^{C \times C}, S_f \in \mathbb{R}^{C \times XY}$, and $V_f \in \mathbb{R}^{XY \times XY}$. 
We then take the rank $1$ approximation of $W_f$, 
%\begin{equation}
%\label{blo1}
	$\tilde{W}_f = \tilde{U}_f \tilde{S}_f \tilde{V}_f^{\top} ~,$
%\end{equation}
where $\tilde{U}_f \in \mathbb{R}^{C \times 1}, \tilde{S}_f \in \mathbb{R}, \tilde{V}_f \in \mathbb{R}^{1 \times XY}$.
We can further exploit the regularity in the weights by sharing the color component basis between different output features. 
We do this by clustering the $F$ left singular vectors, $\tilde{U}_f$, of each output feature $f$ into $C'$ clusters, 
for $C' < F$  . We constrain the clusters to be of equal size as discussed in section \ref{subsec:clustering}.  
Then, for each of the $\frac{F}{C'}$ output features, $f$, that is assigned to cluster $c_f$, we can approximate $W_f$ with
%\begin{equation}
%\label{blo2}
	$\tilde{W}_f = U_{c_f} \tilde{S}_f \tilde{V}_f^{\top}$
%\end{equation}
where $U_{c_f} \in \mathbb{R}^{C \times 1}$ is the cluster center for cluster $c_f$ and $\tilde{S}_f$ and $\tilde{V}_f$ are as before. 

This monochromatic approximation is illustrated in the left panel of Figure \ref{fig:monochromatic}.
Table \ref{table:ops} shows the number of operations required for the standard and monochromatic versions.

\subsection{Biclustering Approximations}\label{subsec:clustering}
%\subsection{Tensor Vector Quantization}\label{subsec:clustering}

We exploit the redundancy within the 4-D weight tensors in the higher convolutional layers
by clustering the filters, such that each cluster can
be accurately approximated by a low-rank factorization. 
We start by clustering the rows of $W_C \in \mathbb{R}^{C \times (XYF)}$, which results in
clusters $C_1, \dots, C_a$. Then we cluster the columns of $W_F  \in
\mathbb{R}^{(CXY) \times F}$, producing clusters $F_1,
\dots, F_b$. These two operations break the original weight tensor $W$
into $ab$ sub-tensors $\{W_{C_i, F_j}\}_{i = 1, \dots, a, j = 1,
  \dots, b}$ as shown in Figure \ref{fig:biclustering}. Each
sub-tensor contains similar elements, and thus is easier to
fit with a low-rank approximation. 

In order to exploit the parallelism inherent in CPU and GPU
architectures it is useful to constrain clusters to be of equal sizes.
We therefore perform the biclustering operations (or clustering 
for monochromatic filters in Section \ref{subsec:monochromatic}) using a modified
version of the $k$-means algorithm which balances the cluster count at
each iteration. 
It is implemented with the Floyd algorithm, by modifying the Euclidean distance
with a subspace projection distance.

After the input and output clusters have been obtained, we find a low-rank approximation of each 
sub-tensor using either the SVD decomposition or the outer product decomposition as described in Section \ref{subsubsec:svd_tensor}.
We concatenate the $X$ and $Y$ spatial dimensions of the sub-tensors so that the decomposition is applied to the 3-tensor, $W_S \in \mathbb{R}^{C \times (XY) \times F}$.
While we could look for a separable approximation along the spatial dimensions as well, we found the resulting gain to be minimal.    
Using these approximations, the target
output can be computed with significantly fewer operations. The number
of operations required is a function the number
of input clusters, $G$, the output clusters $H$ and the rank of the sub-tensor 
approximations ($K_1, K_2$ for the SVD decomposition; $K$ for the outer product decomposition.  
The number of operations required for each approximation is described in Table \ref{table:ops}.

\begin{table}[t]
\centering
\begin{tabular}{|lc|}
\hline
{\bf Approximation technique} & {\bf Number of operations} \\
\hline
\hline
No approximation & $X Y C F N M \Delta^{-2}$\\
Monochromatic & $C' C N M + X Y F N M \Delta^{-2}$\\
Biclustering + outer product decomposition & $G H K (N M \frac{C}{G} + X Y N M \Delta^{-2} + \frac{F}{H} N M \Delta^{-2})$ \\  
Biclustering + SVD & $G H N M (\frac{C}{G}K_1 + K_1 X Y K_2 \Delta^{-2} + K_2\frac{F}{H})$\\
\hline
\end{tabular}
\caption{Number of operations required for various approximation methods.} 
\label{table:ops}
\vspace{-3mm}
\end{table}

\subsection{Fine-tuning}
Many of the approximation techniques presented here can efficiently compress the weights of a CNN with negligible degradation of classification performance provided the approximation is not too harsh.
Alternatively, one can use a harsher approximation that gives greater speedup gains but hurts the performance of the network. In this case, the approximated layer and all those below it can be fixed and the upper layers can be fine-tuned until the original performance is restored.  
