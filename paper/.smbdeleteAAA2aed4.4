\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{overfeat}
\citation{denil2013predicting}
\citation{hintonseparable}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{vanhoucke2011improving}
\citation{mathieu2013fast}
\citation{denil2013predicting}
\citation{zisserman14}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}}
\newlabel{relwork}{{2}{2}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Convolutional Tensor Compression}{2}{section.3}}
\newlabel{sec:approx_tech}{{3}{2}{Convolutional Tensor Compression}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Approximation Metric}{2}{subsection.3.1}}
\newlabel{reconstr_sect}{{3.1}{2}{Approximation Metric}{subsection.3.1}{}}
\citation{zisserman14}
\newlabel{approxi}{{1}{3}{Approximation Metric}{equation.3.1}{}}
\newlabel{poormansmaha}{{2}{3}{Approximation Metric}{equation.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Low-rank Tensor Approximations}{3}{subsection.3.2}}
\newlabel{subsec:low_rank}{{3.2}{3}{Low-rank Tensor Approximations}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Matrix Decomposition}{3}{subsubsection.3.2.1}}
\newlabel{subsubsec:svd}{{3.2.1}{3}{Matrix Decomposition}{subsubsection.3.2.1}{}}
\citation{rankonetensors}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Higher Order Tensor Approximations}{4}{subsubsection.3.2.2}}
\newlabel{subsubsec:svd_tensor}{{3.2.2}{4}{Higher Order Tensor Approximations}{subsubsection.3.2.2}{}}
\newlabel{eq:rank1}{{4}{4}{Higher Order Tensor Approximations}{equation.3.4}{}}
\newlabel{eq:rankK}{{5}{4}{Higher Order Tensor Approximations}{equation.3.5}{}}
\newlabel{fig:monochromatic}{{1(a)}{4}{Subfigure 1(a)}{subfigure.1.1}{}}
\newlabel{sub@fig:monochromatic}{{(a)}{4}{Subfigure 1(a)\relax }{subfigure.1.1}{}}
\newlabel{fig:biclustering}{{1(b)}{4}{Subfigure 1(b)}{subfigure.1.2}{}}
\newlabel{sub@fig:biclustering}{{(b)}{4}{Subfigure 1(b)\relax }{subfigure.1.2}{}}
\newlabel{fig:monochromatic}{{1(c)}{4}{Subfigure 1(c)}{subfigure.1.3}{}}
\newlabel{sub@fig:monochromatic}{{(c)}{4}{Subfigure 1(c)\relax }{subfigure.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  A visualization of monochromatic and biclustering approximation structures. {\bf  (a)} The monochromatic approximation, used for the first layer. Input color channels are projected by a set of intermediate color channels. After this transformation, output features need only to look at one intermediate color channel. This sparsity structure is what makes the speedups feasible. {\bf  (b)} The biclustering approximation, used for higher convolution layers. Input and output features are clustered into equal sized groups. The weight tensor corresponding to each pair of input and output clusters is then approximated. {\bf  (c)} The weight tensors for each input-output pair in (b) is approximated by a sum of rank 1 tensors using techniques described in \ref  {subsubsec:svd_tensor}}}{4}{figure.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Monochromatic Convolution Approximation}{4}{subsection.3.3}}
\newlabel{subsec:monochromatic}{{3.3}{4}{Monochromatic Convolution Approximation}{subsection.3.3}{}}
\newlabel{blo1}{{6}{4}{Monochromatic Convolution Approximation}{equation.3.6}{}}
\citation{zeiler2013visualizing}
\citation{imagenet}
\citation{eigenweb}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Number of operations required for various approximation methods.}}{5}{table.1}}
\newlabel{table:ops}{{1}{5}{Number of operations required for various approximation methods}{table.1}{}}
\newlabel{blo2}{{7}{5}{Monochromatic Convolution Approximation}{equation.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Biclustering Approximations}{5}{subsection.3.4}}
\newlabel{subsec:clustering}{{3.4}{5}{Biclustering Approximations}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{5}{section.4}}
\newlabel{sec:experiments}{{4}{5}{Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Speedup}{5}{subsection.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Evaluation time in seconds per layer on CPU (left) and GPU (right) with batch size of 128. Results are averaged over 8 runs.}}{6}{table.2}}
\newlabel{evaluation_time}{{2}{6}{Evaluation time in seconds per layer on CPU (left) and GPU (right) with batch size of 128. Results are averaged over 8 runs}{table.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}First Layer}{6}{subsubsection.4.1.1}}
\newlabel{fig:RGB_components}{{4.1.1}{7}{First Layer}{subsubsection.4.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visualization of the 1st layer filters in MattNet. Each component of the 96 7x7 filters is plotted in RGB space. Points are colored based on the output filter they belong to. Hence, there are 96 colors and $7^2$ points of each color. ({\bf  Left}) Shows the original filters and ({\bf  Right}) shows the filters after the monochromatic approximation, where each filter has been projected down to a line in colorspace.}}{7}{figure.2}}
\newlabel{fig:denoising}{{4.1.1}{7}{First Layer}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Original and approximate versions (using 12 colors) 