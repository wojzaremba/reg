\section{Experiments}\label{sec:experiments}

We use the convolutional architecture of \cite{zeiler2013visualizing}, trained on the
ImageNet 2012 dataset \cite{imagenet}.
We evaluate these networks on both CPU and GPU platforms. 

We present results showing the performance of the approximations described in section \ref{sec:application} in terms of prediction accuracy, speedup gains and reduction in memory overhead. 

\vspace{-0.3cm}
\subsection{Speedup}
Table \ref{evaluation_time} shows the time breakdown of forward
propagation for each layer in MattNet. Since the majority of time is spent
in the first and second layer convolution operations, we restrict our
attention to these layers. However, our approximations could easily
applied to convolutions in upper layers as well.

We implemented several different CPU and GPU approximation routines 
in an effort to achieve empirical
speedups. Both the baseline and approximation CPU code is implemented
in C++ using Eigen3 library \cite{eigenweb} compiled with Intel MKL.
We also use Intel's implementation of openmp, and multithreading. The
baseline gives comparable performance to highly optimized MATLAB
convolution routines and all of our CPU speedup results are computed
relative to this.  We used Alex Krizhevsky's CUDA convolution routines
\footnote{\url{https://code.google.com/p/cuda-convnet/}} as a baseline for GPU
comparisons. The approximation versions are written in CUDA. All GPU
code was run on a standard nVidia Titan card.

We have found that in practice it is often difficult to achieve
speedups close to the theoretical gains based on the number of
arithmetic operations.
Moreover, different computer architectures and convolutional network
architectures afford different optimization strategies making most
implementations highly specific.  However, regardless of
implementation details, all of the approximations we present reduce
both the number of operations and number of weights required to
compute the output by at least a factor of two.  
%While we present both
%theoretical and empirical speedup results, the empirical aspects
%serves largely as a proof of concept. Our aim is to illustrate that
%the reduction in floating point operations and memory accesses that
%are afforded by the approximations can in fact translate into speedups
%in practice.  Our speedup results on both CPU and GPU are promising,
%especially given that our baselines for comparison comprise of highly
%optimized code. However, we believe that with extensive engineering
%efforts, speedups can be much closer to theoretically optimal levels.

\begin{table}
\tiny
\parbox{.45\linewidth}{
\centering
\begin{tabular}{rrr}
\hline
Layer & Time per batch (sec) & Fraction \\
\hline
Conv1 & $2.8317 \pm 0.1030 $ & 21.97\% \\
MaxPool & $0.1059 \pm 0.0154$ & 0.82\% \\
LRNormal & $0.1918 \pm 0.0162$ & 1.49\% \\
Conv2 & $4.2626 \pm 0.0740 $ & 33.07\% \\
MaxPool & $0.0705  \pm 0.0029$ & 0.55\% \\
LRNormal & $0.0772\pm 0.0027$ & 0.60\% \\
Conv3 & $1.8689\pm 0.0577$ & 14.50\% \\
MaxPool & $0.0532\pm 0.0018 $ & 0.41\% \\
Conv4 & $1.5261\pm 0.0386$ & 11.84\% \\
Conv5 & $1.4222\pm 0.0416$& 11.03\% \\
MaxPool & $0.0102\pm 0.0006 $ & 0.08\% \\
FC & $0.3777\pm 0.0233$ & 2.93\% \\
FC & $0.0709  \pm 0.0038$ & 0.55\% \\
FC & $0.0168 \pm 0.0018$ & 0.13\% \\
Softmax & $0.0028 \pm 0.0015$ & 0.02\%\\
\hline 
Total & $12.8885$ & \\
\hline
\end{tabular}
}
\parbox{.45\linewidth}{
\centering
\begin{tabular}{rrr}
\hline
Layer & Time per batch (sec) & Fraction \\
\hline
Conv1 & $0.0604 \pm 0.0112$ & 5.14\% \\
MaxPool & $0.0072 \pm 0.0040$ & 0.61\% \\
LRNormal & $0.0041 \pm 0.0043$ &  0.35\% \\
Conv2 & $0.4663 \pm 0.0072$ & 39.68\% \\
MaxPool & $0.0032 \pm 0.0000$ &  0.27\% \\
LRNormal & $0.0015 \pm 0.0003$ & 0.13\% \\
Conv3 & $0.2219 \pm 0.0014$ & 18.88\% \\
MaxPool & $0.0016 \pm 0.0000$ & 0.14\% \\
Conv4 & $0.1991 \pm 0.0001$ & 16.94\% \\
Conv5 & $0.1958 \pm 0.0002$ &  16.66\% \\
MaxPool & $0.0005  \pm 0.0001$ & 0.04\% \\
FC & $0.0077 \pm 0.0013$ & 0.66\% \\
FC & $0.0017 \pm 0.0001$ & 0.14\% \\
FC & $0.0007 \pm 0.0002$  & 0.06\% \\
Softmax & $0.0038 \pm 0.0098$ & 0.32\%\\
\hline 
Total & 1.1752 & \\
\hline
\end{tabular}
}
\caption{Evaluation time in seconds per layer on CPU (left)
  and GPU (right) with batch size of 128. Results are averaged over 8
  runs.} 
\label{evaluation_time}
\end{table}
%\vspace{-0.8cm}

\subsubsection{First Layer}

The first convolutional layer has 3 input channels, 96
output channels and 7x7 filters.  We approximated the weights in this
layer using the monochromatic approximation described in section
\ref{sec:monochromatic}. The monochromatic approximation works well if
the color components span a small number of one dimensional
subspaces. Figure \ref{fig:RGB_components} illustrates the RGB components of the original filters and the filters after
projecting onto one dimensional subspaces. Figure \ref{fig:denoising} gives an alternate
presentation of this approximation, showing the original first layer
filters beside their monochromatic approximations.  Interestingly, we
notice that the approximated filters often appear to be cleaned up
versions of the original filters. This leads up to believe that the
approximation techniques presented here might be viable strategies of
cleaning up or denoising weights after training, potentially improving generalization performance.

\begin{figure}[t]
\centering
\begin{minipage}{0.75\textwidth}
	\includegraphics[width=0.5\linewidth]{img/RGB_components_matthew_3d.eps}
	\quad\quad
	\includegraphics[width=0.5\linewidth]{img/monochromatic_RGB_components_matthew_3d.eps} 
\end{minipage}
\vspace{-3mm}
\label{fig:RGB_components}
\caption{Visualization of the 1st layer filters in MattNet. Each component of the 96 7x7 filters is plotted in RGB space. Points are colored based on the output filter they belong to. Hence, there are 96 colors and $7^2$ points of each color. ({\bf Left}) Shows the
  original filters and ({\bf Right}) shows the filters after the monochromatic approximation, where each filter has been projected down to a line in colorspace.}
\end{figure}



\begin{figure}[t]
\begin{center}
      \includegraphics[width=0.9\linewidth]{img/denoising_combined.pdf}
\end{center}
\label{fig:denoising}
\vspace{-6mm}
\caption{Original and approximate versions (using 12 colors) of 1st later filters from MattNet.}
\end{figure}

We evaluated the network on 20K validation images from the ImageNet12 dataset using various monochromatic approximations for the first layer weights. 
The only parameter in the approximation is $C'$, the number of color channels used for the intermediate representation. As expected, the network performance begins to degrade as $C'$ decreases. 
%Table \ref{table:mono_perf} shows how the classification performance degrades for increasing $C'$. 

%\begin{table}[t]
%\tiny
%\centering
%\begin{tabular}{ccc}
%\hline
%Number of colors \hspace{2mm} & Increase in test error & \hspace{2mm}Theoretical speedup\\
%\hline
%6 & 10.1929\% & 2.95$\times$ \\
%8 & 4.0528\% & 2.94$\times$\\
%12 & 2.1973\% & 2.91$\times$\\
%16 & 1.1475\% & 2.88$\times$\\
%24 & 1.7212\% & 2.82$\times$\\
%48 & 0.5249\% & 2.66$\times$\\
%96 & 0.5738\% & 2.39$\times$ \\
%\hline 
%\end{tabular}
%\caption{Performance of MattNet when first layer weights are replaced with monochromatic approximation and the corresponding theoretical speedup. Classification error on 8K validation images tends to increase as the approximation becomes harsher (i.e. fewer colors are used). Theoretical speedups vary only slightly as the number of colors used increases since the color transformation contributes relatively little to the total number of operations.} 
%\label{table:mono_perf}
%\end{table}


%Theoretically achievable speedups can measured in terms of the number of floating point operations required to compute the output of the convolution. 
%For the monochromatic approximation this theoretical speedup with respect to MattNet is given in table \ref{table:mono_perf}. 
%The majority of the operations result from the convolution part of the computation. In comparison, the number of operations required for the color transformation is negligible. Thus, the theoretically achievable speedup decreases only slightly as the number of color components used is increased. 

The number of FLOPS required to compute the output of the monochromatic convolution is reduced by a factor of $2-3\times$, with the larger gain resulting for small $C'$. 
Since the majority of the operations result from the convolution part of the computation, rather than the color transform,
the theoretically achievable speedup decreases only slightly as $C'$ is increased.
In practice, we found it difficult to optimize the monochromatic convolution routines as for large $C'$ since less work can be shared amongst output filters making it challenging to parallelize.
Figure \ref{fig:mono_speedups} shows the empirical speedups we achieved on CPU and GPU and the corresponding network performance for various numbers of colors used in the monochromatic approximation.   
Our CPU and GPU implementations achieve empirical speedups of $2-2.5\times$ relative to the baseline with less than 1\% drop in classification performance. 

\begin{figure}[t]
\centering
\begin{minipage}{0.9\textwidth}
      \includegraphics[width=0.5\linewidth]{img/layer1_CPUspeedup_vs_performance_loss_finetune_and_orig.pdf}
	\quad\quad
      \includegraphics[width=0.5\linewidth]{img/layer1_GPUspeedup_vs_performance_loss_finetune_and_orig.pdf}
\end{minipage}
\caption{Empirical speedups on ({\bf Left}) CPU and ({\bf Right}) GPU for the first layer. $C'$ is the number of colors used in the approximation.}
\label{fig:mono_speedups}
\end{figure}

\subsubsection{Second Layer}
The second convolutional layer has 96 input channels, 256 output channels and 5x5 filters. 
We approximated the weights using the techniques described in section \ref{subsec:clustering}. 
We evaluated the original and approximate convolution operation in
this layer using 20K validation images from the ImageNet12 dataset. 
We explored various configurations of the approximations by varying the number of input clusters $G$, the number of output clusters $H$ and the rank of the approximation (denoted by $K_1$ and $K_2$ for the SVD decomposition and $K$ for the outer product decomposition). 

%We can measure the theoretically achievable speedups for a particular approximation in terms of the number of floating point operations required to compute the output of the convolution. 
%Figure \ref{fig:biclustering_theory} plots the theoretically achievable speedups against the drop in classification performance for various configurations of the biclustering with outer product decomposition technique.  
%For a given setting of input and output clusters numbers, the performance tends to degrade as the rank is decreased. 

%In practice we found it difficult to achieve speedups close to theoretically optimal levels. 
Figure \ref{fig:biclust_speedups} shows our empirical speedups on CPU
and GPU and the corresponding network performance for
various approximation configurations. For the CPU implementation we used the biclustering with SVD approximation. For the GPU implementation we using the biclustering with outer product decomposition approximation.  
We achieved promising results and present speedups of $2-2.5\times$ relative to the baseline with less than a 1\% drop in performance.
%\begin{figure}[t]
%\centering
%\begin{minipage}{0.75\textwidth}
%  \includegraphics[width=0.5\linewidth]{img/layer2_theoreticalspeedup_vs_performance_loss.pdf} 
%  \quad\quad
%  \includegraphics[width=0.5\linewidth]{img/layer2_theoreticalspeedup_vs_performance_loss_pierre.pdf} 
%\end{minipage}
%\caption{Theoretically achievable speedups with various biclustering approximations applied to ({\bf Left}) MattNet and ({\bf Right}) PierreNet.}
%\label{fig:biclustering_theory}
%\end{figure}

\begin{figure}[t]
\centering
\begin{minipage}{0.9\textwidth}
      \includegraphics[width=0.5\linewidth]{img/layer2_CPUspeedup_vs_performance_loss_finetune_and_orig.pdf}
      \quad\quad
      \includegraphics[width=0.5\linewidth]{img/layer2_GPUspeedup_vs_performance_loss_finetune_and_orig.pdf}
\end{minipage}
\caption{Empirical speedups for second convolutional layer. ({\bf Left}) Speedups on CPU using biclustered ($G = 2$ and $H = 2$) with SVD approximation.({\bf Right}) peedups on GPU using biclustered ($G = 48$ and $h = 2$) with outer product decomposition approximation.}
\label{fig:biclust_speedups}
\end{figure}


\subsection{Reduction in memory overhead}
In many commercial applications memory conservation and storage are a
central concern. This mainly applies to embedded systems
(e.g. smartphones), where available memory is limited, and users are
reluctant to download large files. In these cases, being able to
compress the neural network is crucial for the viability of the
product. 

In addition to requiring fewer operations, our approximations
require significantly fewer parameters when compared to the original
model. 
Since the majority of parameters come from the fully connected layers, we include these layers in our analysis of memory overhead. We compress the fully connected layers using standard SVD as described in \ref{\subsubsec:svd_tensor}. We use $K$ to denote the rank of the approximation. 
Table \ref{table:memory} shows the number of parameters for
various approximation methods as a function of hyperparameters for the approximation techniques.
The table also shows the empirical reduction of parameters and the corresponding network performance for specific instantiations of the approximation parameters.

\begin{table}[t]
\tiny
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
{\bf Approximation method} & {\bf Number of parameters} & {\bf Approximation} & {\bf Reduction} & {\bf Increase in}\\ 
& & {\bf hyperparameters} &  {\bf in weights} & {\bf test error}\\
\hline
\hline
Standard colvolution & $CXYF$ & & &\\
\hline
Conv layer 1: Monochromatic & $CC' + XYF$ & $C' = 6$ & $3\times$ & 0.43\%\\
\hline
Conv layer 2: Biclustering & $GHK (\frac{C}{G} + XY + \frac{F}{H})$ & $G = 48$; $H = 2$; $K = 6$ & 5.3$\times$ & 0.68\%\\
	    + outer product decomposition  & &  & &\\
\hline
Conv layer 2: Biclustering + SVD& $G H (\frac{C}{G}K_1 + K_1 X Y K_2 + K_2 \frac{F}{H})$ & $G = 2; H = 2$; $K_1 = 19$; $K_2 = 24$ & $3.9\times$ & 0.9\% \\
\hline
Standard FC & $N M$ & & &\\
\hline
FC layer 1: Matrix SVD & $NK + KM$ & $K = 250$ & $13.4\times$ & 0.8394\%\\
                      & & $K = 950$ & $3.5\times$ & 0.09\%\\
\hline 
FC layer 2: Matrix SVD & $NK + KM$ & $K = 350 $ & $5.8\times$ & 0.19\%\\
                      & & $K = 650$ & $3.14\times$ & 0.06\%\\
\hline 
FC layer 3: Matrix SVD & $NK + KM$ & $K = 250$ & $8.1\times$ & 0.67\%\\
                      & & $K = 850$ & $2.4\times$ & 0.02\%\\
\hline 
\end{tabular}
\caption{Number of parameters expressed as a function of hyperparameters for various approximation methods and empirical reduction in parameters with corresponding network performance.} 
\label{table:memory}
\end{table}

