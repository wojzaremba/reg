\section{Discussion}
In this paper we have presented techniques that can speed up the
bottleneck convolution operations in the first layers of a CNN by a factor $2-3\times$, with negligible loss of performance ($< 1\%$). The empirical speedups
achieved are still some way short of the theoretical gains, thus
further improvements might be expected with further engineering effort. 
Moreover, the techniques are orthogonal
to other approaches for efficient evaluation, such as quantization or
working in the Fourier domain. Hence, they can potentially be used
together to obtain further gains. 

Given the widespread use of neural networks in large scale industrial
settings, the speed gains we demonstrate have significant economic
value. Companies such as Google or Facebook process $10^8$ images/day,
requiring many thousands of machines. Our speed gains mean that the
number of machines dedicated to running convolutional networks could be significantly reduced, saving millions of
dollars, as well as significantly reducing the environmental impact.

We also show that our methods reduce the memory footprint of weights
in the first two layers by factor of $2-3\times$ and the fully connected layers by a factor of $5-13\times$.
Since the vast majority of weights reside in the fully connected layers, compressing only these layers 
translate into a significant savings, which would facilitate mobile deployment
of convolutional networks.  

%Our approximations are applied to fully trained networks. The
%small performance drops that result could well be mitigated by further
%training the network after applying the approximation. By alternating
%these two steps, we could potentially achieve further gains in
%speed-up for a modest performance drop.

An interesting avenue of research to explore in further work is the ability of these techniques to aid in regularization either during or post training.
The low-rank projections effectively decrease number of learnable parameters,
suggesting that they might improve generalization ability,
a major issue with large CNNs.
The regularization potential of the low-rank approximations is further motivated by two observations. 
The first is that the approximated filters for the first conolutional layer appear to be cleaned up versions of the original filters. 
Additionally, we noticed that we sporadically achieve better
test error with some of the more conservative approximations. 


% Presented methods were applied to fully trained networks. Obviously, networks were not trained
% with constrains on connectivity or sparsity as are present in approximated network. This means 
% that agresive approximations, which can have much larger speedup are likely to give worse error score. 
% Training with new connectivity or sparsity constraint might be quite difficult. Constrains 
% comming from approximation would have to be included in optimization, and a lot of additional coding would be
% neccessary. We propose different strategy. 


% It is easy to decompose weights using any of our method, and then reconstruct them. Particularly,
% reconstructed weights can be approximated exactly with the same decomposition. Let's call this process
% projection. Proposed strategy is to take a fully trained network, and iterativelly repeat (1) apply projection, 
% (2) train for few epochs. Hopefuly, this way weights will become expressable in desired way, which can be speededup
% more than $2$ times, and prediction score would be preserved due to fine-tunning. From engineering perspective
% stage (1) can be implemented in any external easy environment (e.g. MATLAB), while stage can stay as it is. 
% Stage (1) is executed every few epochs, so it is not a computational bottle-neck.



% and thus our speed gains 

% the speed gains we demonstrate have 
% Speeding up the evaluation of neural networks has a high industrial impact. Computation
% of forward pass on all public internet available images (billions of images), or
% on all social media data cost in milions of dollars for a single pass. High costs
% of evaluation limits use of neural networks, and might forces companies to rely
% on inferior object recognition techniques which are much cheaper in evaluation 
% (e.g. SIFT features). 



% Presented in this paper techniques show how to achieve speedup by the factor of $2$,
% with moderate loss of performance ($< 1\%$). Moreover, they are ortogonal 
% to previously developed techniques like quantization, or working in Fourier domain, and
% can be used together to further boost evaluation speed. Finally, we believe that fine-tunning
% technique described in Section \ref{sec:fine_tunning} can enable even further 
% agresive approximations, and give a higher boost.



%\input{fine_tunning}



